# NERSC Weekly Email, Week of December 9, 2019 #

## Contents ## 

- [Summary of Upcoming Events and Key Dates](#dates)

- [NUG Monthly Webinar Scheduled for this Thursday, December 12 at 11 am](#nugwebinar)
- [New Allocation Year Begins January 14, 2020](#neway)
- [Women in HPC Summit Call for Submissions](#whpc)
- [New Community File System to Replace Project File System in New Allocation Year](#cfs)
- [ALCC Letters of Intent Due December 16](#alcc)
- [UPC++ Workshop on December 16 at NERSC](#upcpp)
- [New Programing Environments Now Available on Cori](#corisw)
- [Dynamic Linking to Become Default on Jan 14, 2020; Test Now!](#dynamic)
- [User Dotfile Changes Planned for February 2020](#dotfiles)
- [PASC20 Submissions Due this Sunday!](#pasc)
- [Call For Papers: Performance, Portability, and Productivity in HPC Forum (P3HPC)](#p3hpc)
- [NERSC Will Support Only Python3 in New Allocation Year](#python2)
- [FY19 User Statisitics Sent to DOE](#stats)
- [This Week on "NERSC User News" Podcast: The Community File System](#podcast)
- [Come Work for NERSC!](#careers)
- [Upcoming Outages](#outages)
- [About this Email](#about)

## Summary of Upcoming Events and Key Dates <a name="dates"/></a> ##

            December 2019
     Su  Mo  Tu  We  Th  Fr  Sa
      1   2   3   4   5   6   7  
      8   9  10  11 *12**13* 14   12 Dec         NUG Monthly Webinar [1]
                                  13 Dec         ERCAP Awards Announced [2]
                                  13 Dec         WHPC Tutorial Submissions Due [3]
    *15**16* 17  18  19 *20* 21   15 Dec         PASC20 Submissions Due [4]
                                  16 Dec         ALCC LOI Closes [5]
                                  16 Dec         UPC++ tutorial [6]
                                  20 Dec         WHPC Paper Submissions Due [7]
     22  23 *24--25--26--27--28-  24 Dec- 1 Jan  Winter Holiday [8]
    -29--30--31-

            January  2020
     Su  Mo  Tu  We  Th  Fr  Sa
                 -1*  2   3   4
      5   6   7   8   9  10  11
     12  13 *14* 15  16  17  18   14 Jan         AY2020 Begins [9]
     19 *20* 21  22  23 *24* 25   20 Jan         MLK Holiday [10]
                                  24 Jan         P3HPC Submissions due [11]
     26  27  28  29  30 *31*      31 Jan         WHPC Poster Submissions due [12]

            February 2020   
     Su  Mo  Tu  We  Th  Fr  Sa
                              1 
      2  *3*  4   5   6   7   8    3 Feb         ALCC Due Date [13]
      9  10  11  12  13  14  15 
     16 *17* 18  19  20  21  22   17 Feb         Presidents Day Holiday [14]
     23  24  25  26  27  28  29

Notes:

1. **December 12, 2019**: [NUG Monthly Webinar](#nugwebinar)
2. **December 13, 2019**: ERCAP Awards Announced (approximate date)
3. **December 13, 2019**: [Women in HPC Summit tutorial submissions due](#whpc)
4. **December 15, 2019**: [PASC20 Submissions due](#pasc)
5. **December 16, 2019**: [ALCC Letters of Intent Closes](#alcc)
6. **December 16, 2019**: [UPC++ Tutorial](#upcpp)
7. **December 20, 2019**: [Women in HPC Summit paper submissions due](#whpc)
8. **December 24, 2019-January 1, 2020**: Christmas/New Year Holiday (Limited Consulting or Account Support)
9. **January 14, 2020**: First day of Allocation Year 2020
10. **January 20, 2020**: Martin Luther King Jr. Day Holiday (No Consulting or Account Support)
11. **January 24, 2020**: [P3HPC Submissions due](#p3hpc)
12. **January 31, 2020**: [Women in HPC Summit poster submissions due](#whpc)
13. **February 3, 2020**: [ALCC Proposals due](#alcc)
14. **February 17, 2020**: Presidents Day Holiday (No Consulting or Account Support)
15. All times are **Pacific Time zone**


### Other Significant Dates ###

- **April 7-9, 2020**: [Performance, Portability, and Productivity in HPC Forum](https://p3hpcforum2020.alcf.anl.gov/)
- **April 29-May 1, 2020**: [Women in HPC Summit](https://womeninhpc.org/events/summit-2020)
- **May 25, 2020**: Memorial Day Holiday (No Consulting or Account Support)
- **July 4, 2020**: Independence Day Holiday (No Consulting or Account Support)
- **September 7, 2020**: Labor Day Holiday (No Consulting or Account Support)
- **November 26-27, 2020**: Thanksgiving Holiday (No Consulting or Account Support)
- **December 24, 2020-January 1, 2021**: Christmas/New Year Holiday (Limited Consulting or Account Support)


## NUG Monthly Webinar Scheduled for this Thursday, December 12 at 11 am <a name="nugwebinar"/></a> ##

The monthly NERSC Users Group (NUG) webinar has been scheduled for **this
Thursday, November 12, at 11 am Pacific**. The agenda includes an introduction
to the new Community File System (CFS), and using Dask on Jupyter at NERSC.

For more information, including connection info, please see 
<https://www.nersc.gov/users/NUG/teleconferences/nug-webinar-dec-12-2019/>.


## New Allocation Year Begins January 14, 2020 <a name="neway"/></a> ##

The 2020 Allocation Year (AY) begins on January 14, 2020. There will be several
changes that will take effect in the new AY. Of note:
- The software environment defaults will change, as outlined in the [NERSC CDT
Policy](https://docs.nersc.gov/programming/Cray-PE-CDT-policy/);
- The default Python module will point to a version of Python 3;
- The new Community File System (CFS) will replace the Project File System.

More details about these and other changes will be provided in the next several
weeks.


## Women in HPC Summit Call for Submissions <a name="whpc"/></a> ##

Submissions for papers, posters, and tutorials are now being accepted for the
first Women in HPC Summit, to be held April 29-May 1, 2020 in Vancouver, British
Columbia, Canada.

Papers, posters, and tutorials are solicited on a diverse range of technical and
diversity, inclusion, and leadership topics, including but not limited to:
- Programming models and applications for HPC, Big Data, and AI;
- Architectures and accelerators on high-performance platforms;
- Computational models and algorithms for HPC, Big Data, and AI;
- Using machine learning to analyze large-scale systems;
- Performance modeling, analysis, and benchmarking of HPC, Big Data, and AI
applications/architectures;
- Methods and techniques to create a diverse workforce;
- Inclusive leadership and retention strategies;
- Building diversity advocates and allies;
- Dealing with unconscious bias and sexism in the workplace;
- Fostering creativity through diversity.

Tutorial submissions are due this Friday, December 13, 2019, AOE. Paper
submissions are due on Friday, December 20, 2019, AOE, and poster submissions
are due January 31, 2020, AOE. For more information and to submit, please see
<https://womeninhpc.org/events/summit-2020>.


## New Community File System to Replace Project File System in New Allocation Year <a name="cfs"/></a> ##

The new "Community" File System (CFS) will replace the Project file system in
the new allocation year (AY). No action is required for users; NERSC will
transfer your data from Project to CFS.

Each active repository will have a directory on CFS with the path structure
`/global/cfs/cdirs/<repo_name>`, but existing 
`/global/project/projectdirs/<repo_name>` paths will redirect to the
corresponding CFS path until mid-2020.

This week's [NUG meeting](https://www.nersc.gov/users/NUG/teleconferences/nug-webinar-dec-12-2019/)
includes more information about CFS and the migration timeline, and this week's
NERSC User News [podcast](https://anchor.fm/nersc-news/episodes/Community-File-System-Kristy-Kallback-Rose--Greg-Butler--and-Ravi-Cheema-Interview-e9d88q/a-a149hf5) 
is on the topic of CFS.


## ALCC Letters of Intent Due December 16 <a name="alcc"/></a> ##

For the 2020-2021 ALCC campaign, PIs are **required** to submit a Letter of 
Intent (LOI) as the first step of submitting a proposal. The ALCC submission 
site (<https://science.osti.gov/ascr/ALCC>) opened October 1,
with LOIs due on December 16. 

All proposals are due when the submission site closes at 11:59 PM (Eastern Time)
on February 3, 2020. Information from the LOI will be automatically populated to
the proposal by December 18. Details about submission requirements can be found 
at <https://science.osti.gov/ascr/Facilities/Accessing-ASCR-Facilities/ALCC>.


## UPC++ Workshop on December 16 at NERSC <a name="upcpp"/></a> ##

This event is a repeat of the tutorial delivered on November 1, but with the 
restoration of the hands-on component which was omitted due to uncertainty 
surrounding the power outage at NERSC.

UPC++ is a C++11 library providing classes and functions that support
Partitioned Global Address Space (PGAS) programming. UPC++ provides mechanisms
for low-overhead one-sided communication, moving computation to data through
remote-procedure calls, and expressing dependencies between asynchronous
computations and data movement. It is particularly well-suited for implementing
elaborate distributed data structures where communication is irregular or
fine-grained. The UPC++ interfaces are designed to be composable and similar to
those used in conventional C++. The UPC++ programmer can expect communication to
run at close to hardware speeds.

In this tutorial we will introduce basic concepts and advanced optimization
techniques of UPC++. We will discuss the UPC++ memory and execution models and
walk through implementing basic algorithms in UPC++. We will discuss irregular
applications and how to take advantage of UPC++ features to optimize their
performance.  The tutorial will include hands-on exercises with basic UPC++
constructs. 

Though registration has closed for on-site participation, one may still choose
to audit remotely by [registering](https://www.eventbrite.com/e/ecpnersc-upc-tutorial-tickets-80937832235) 
by this Thursday, December 12.


## New Programing Environments Now Available on Cori <a name="corisw"/></a> ##

During last week's scheduled Cori maintenance, NERSC installed the new Cray 
Programming Environment Software release CDT/19.11. While there was no software 
default version changes made, CDT/19.11 is expected to become the default in the
new allocation year and [dynamic linking](#dynamic) will become default. Please 
see the detailed list of new software at
<https://docs.nersc.gov/systems/cori/timeline/default_PE_history/2019Dec-2020Jan>

Cray compiler users shoule note the important information about the all-new CCE 
9.0 on the above webpage; in particular the CCE 9.0 C/C++ compiler is based on 
Clang instead of the classic Cray compiler. Key consequences of these changes 
include:
- CCE 9.0 compilers are not compatible with pre-CDT-19.06 library versions (such
as MPI);
- The OpenMP flag is no longer turned on by default.


## Dynamic Linking to Become Default on Jan 14, 2020; Test Now! <a name="dynamic"/></a> ##

We plan to set the new CDT/19.11 as default (assuming no major issues are 
discovered beforehand) at the Allocation Year Rollover on **January 14, 2020**.

When this happens, the **default linking mode on Cori will change from static to
dynamic**. We encourage users to test this change now and [let us know](https://help.nersc.gov) 
if you encounter any issues. 

We encourage you to start testing dynamic linking with:

```
% module load cdt/19.11 
% export LD_LIBRARY_PATH=$CRAY_LD_LIBRARY_PATH:$LD_LIBRARY_PATH 
```
and proceeding to compile and run your code.

If, after the default changes, you prefer to use static linking as default 
(e.g., for workflow or performance reasons), you can set: 

```
% export CRAYPE_LINK_TYPE=static   
```
to retain static linking.


## User Dotfile Changes Planned for February 2020 <a name="dotfiles"/></a> ##

Currently, by default, `.bashrc`/`.bash_profile`/`.cshrc`/`.login` files are 
symlinks in your `$HOME` to read-only NERSC-supplied startup files. You may have
made changes to your starting environment by adding 
`.bashrc.ext`/`.bash_profile.ext`/`.cshrc.ext`/`.login.ext` files.

To reduce some shell startup overhead, and to bring NERSC in line with most 
other HPC centers, we will migrate away from this arrangement during the 
scheduled maintenance in **February 2020**. After the change is made, you will 
be able to edit `.bashrc` (etc) directly.

During the change, your `.bashrc` (etc), which is currently a symlink, will be 
replaced by a template `.bashrc` (etc) that simply sources your `.bashrc.ext` 
(etc).

For most users this should have no other impact. But some non-default 
environments and workflows might experience some changes to their environment. 
You can test the changes now, by using the `dotmgr` command and logging in to 
cori12 or dtn12, which now have the new configuration:

    `dotmgr -l` # list my current dotfiles
    `dotmgr -s` # save my current dotfiles, and print the location
    `dotmgr -e` # replace my existing dotfiles with the new arrangement

You can then login to cori12 and/or dtn12 to check whether this affected your 
environment. Check that things still look the same and your aliases still work.

    `ssh cori12`

You can then return your dotfiles to the current configuration with:

    `dotmgr -r <directory-that-the-save-step-returned>`

Note that `dotmgr -e` and `dotmgr -r` **don't affect your current environment** 
-- they affect the contents of your dotfiles. For the changes to take effect, 
you must log out and log back in.

For detailed help, please see <https://docs.nersc.gov/environment/>.
Please let us know of any problems you encounter, by filing a ticket at 
<https://help.nersc.gov>.


## PASC20 Submissions Due this Sunday! <a name="pasc"/></a> ##

The Platform for Advanced Scientific Computing (PASC) invites research paper 
submissions for PASC20, co-sponsored by the Association for Computing Machinery 
(ACM) and SIGHPC, which will be held at the University of Geneva, Switzerland, 
from June 29 to July 1, 2020.

PASC20 is the seventh edition of the PASC Conference series, an international 
platform for the exchange of competences in scientific computing and 
computational science, with a strong focus on methods, tools, algorithms, 
application challenges, and novel techniques and usage of high performance 
computing. The technical program is centered around eight
scientific domains, including chemistry/materials, climate/weather, computer
science/applied math, life sciences, physics, solid earth dynamics, engineering,
and emerging application domains.

The final deadline for submissions is **this Sunday, December 15, 2019**.
For more information on PASC20, including submissions, please see
<https://pasc20.pasc-conference.org>.


## Call For Papers: Performance, Portability, and Productivity in HPC Forum (P3HPC) <a name="p3hpc"/></a> ##

The call for papers for the Performance, Portability, and Productivity in HPC 
Forum (P3HPC) is now open. This workshop is an opportunity for
researchers to share ideas, practical experiences, and methodologies for 
tackling the compelling problems that lie at the intersection of performance, 
portability and productivity.
 
We are particularly interested in research that addresses the complexities of 
real-life applications and/or realistic workloads, the composability challenges 
arising from the use of bespoke solutions, and the desire to "future-proof" 
applications in the long term.

Submissions close January 24, 2020. For more information and to submit a paper,
please see <https://p3hpcforum2020.alcf.anl.gov/>.


## NERSC Will Support Only Python3 in New Allocation Year <a name="python2"/></a> ##

Python 2 will reach its end of life on [January 1, 
2020](https://devguide.python.org/#status-of-python-branches),
at which point there will be no more development, bug fixes, patches, etc.

Therefore, upon the beginning of the 2020 Allocation Year at NERSC, the 
following changes to Python support will occur at NERSC:
- At the AY rollover, the default Python module will become a module based on a
Python 3 distribution.
- The old Python 2 module will remain available for use but users must specify
the version suffix.
- No new installations of Python 2 packages or modules will be performed.
- During the next Cori operating system upgrade, which could occur sometime in
2020, the Python 2 module will be retired.

NERSC will actively support only Python 3 (or future Python versions should
Python 3 become deprecated) on Perlmutter and future systems.

Please let us know your questions via a ticket to <https://help.nersc.gov>.


## FY19 User Statisitics Sent to DOE <a name="stats"/></a> ##

The U.S. Department of Energy Office of Science (SC), which is the primary
sponsor of  NERSC, requires that a limited set of information relating to your
user project/experiment be transmitted to SC at the conclusion of the current
fiscal year. A subset of this information, including your name, institutional
affiliation(s), and project title(s), will be publicly disseminated as part of
an SC user facility user projects/experiments database on the SC website,
http://science.osti.gov, after the conclusion of the fiscal year. For
proprietary projects, SC requests that the user provide a project title that is
suitable for public dissemination. 


## This Week on "NERSC User News" Podcast: The Community File System <a name="podcast"/></a> ##

In this week's NERSC User News podcast, join NERSC Storage Systems Group staff
Kristy Kallback-Rose, Greg Butler, and Ravi Cheema in a discussion about the
new Community File System (CFS): why we're bringing it to users, how your data
will be transferred over, and tips for using the new file system.

The NERSC User News podcast, produced by the NERSC User Engagement Group, is 
available at <https://anchor.fm/nersc-news> and syndicated through iTunes, 
Google Play, Spotify, and more. A direct link to this week's podcast is 
<https://anchor.fm/nersc-news/episodes/Community-File-System-Kristy-Kallback-Rose--Greg-Butler--and-Ravi-Cheema-Interview-e9d88q/a-a149hf5>.

Please give it a listen, and let us know what you think via a ticket at
<https://help.nersc.gov>.


## Come Work for NERSC! <a name="careers"/></a> ##

NERSC currently has several openings for postdocs, system administrators, and 
more! If you are looking for new opportunities, please consider the following 
openings:

- [Computer Systems Engineer](https://jobs.lbl.gov/jobs/computer-systems-engineer-2357):
Help prepare Exascale Computing Project (ECP) codes for the next-generation 
pre-exascale and exascale high performance computing (HPC) systems. 
- Application Performance Specialists for [NESAP](https://jobs.lbl.gov/jobs/application-performance-consultant-1010) 
and [ECP](https://jobs.lbl.gov/jobs/application-performance-specialist-2312):
Help prepare large-scale scientific codes for next-generation high performance 
computing (HPC) systems.
- [High Performance Computing Security Developer](https://jobs.lbl.gov/jobs/high-performance-computing-security-developer-2295):
Protect Exascale class systems in an open science environment and enhance 
network and host intrusion prevention as we migrate from 100G to Terabit 
networks.
- [Software Engineer (Storage and I/O)](https://jobs.lbl.gov/jobs/software-engineer-storage-and-i-o-2275):
Enable DOE researchers and the broader science community to benefit from 
improvements to HDF5 and other leading high-performance computing (HPC) storage 
and I/O software.
- [Data Management Engineer](https://jobs.lbl.gov/jobs/data-management-engineer-2129):
Provide a variety of engineering support services to manage a data warehouse and
notification infrastructure for the NERSC computational facility.
- [NESAP for Simulations Postdoctoral Fellow](https://jobs.lbl.gov/jobs/nesap-for-simulations-postdoctoral-fellow-2004):
work in multidisciplinary teams to transition simulation codes to NERSC's new 
Perlmutter supercomputer and produce mission-relevant science that truly pushes 
the limits of high-end computing.
- [NESAP for Data Postdoctoral Fellow](https://jobs.lbl.gov/jobs/nesap-for-data-postdoctoral-fellow-2003):
work in multidisciplinary teams to transition data-analysis codes to NERSC's new
Perlmutter supercomputer and produce mission-relevant science that truly pushes 
the limits of high-end computing.
- [NESAP for Learning Postdoctoral Fellow](https://jobs.lbl.gov/jobs/nesap-for-learning-postdoctoral-fellow-1964):
work in multidisciplinary teams to develop and implement cutting-edge machine 
learning/deep learning solutions in codes that will run on NERSC's new
Perlmutter supercomputer and produce mission-relevant science that truly pushes 
the limits of high-end computing.
- [HPC Storage Systems Analyst](https://jobs.lbl.gov/jobs/hpc-storage-systems-analyst-1851):
Help architect, deploy, and manage NERSC's storage hierarchy (including Burst
Buffer, Lustre, and Spectrum Scale filesystems, and HPSS archives).

(**Note:** We have received reports that the URLs for the jobs change without 
notice, so if you encounter a page indicating that a job is closed or not found, 
please check by navigating to <https://lbl.referrals.selectminds.com/>, 
scrolling down to the 9th picture that says "All Jobs" and clicking on that. 
Then, under "Business," select "View More" and scroll down until you find the
checkbox for "NE-NERSC" and select it.)

We know that NERSC users can make great NERSC employees! We look forward to 
seeing your application.


## Upcoming Outages <a name="outages"/></a> ##

- **HPSS Regent (Backup)** 
    - 12/11/19 9:00-13:00 PST, Unavailable
    - 12/17/19 5:30-12/19/19 8:00 PST, Scheduled Maintenance
               Regent System will be moving from the Oakland facility to the
               NERSC machine room at the Berkeley Lab.
- **Spin**
    - 12/18/19 10:00-18:00 PST, Scheduled Maintenance
               Services will be down briefly (1-2 min) within the window for
               network configuration changes to increase performance between
               containers.

Visit <http://www.nersc.gov/users/live-status/> for latest status and outage 
information.


## About this Email <a name="about"/></a> ##

You are receiving this email because you are the owner of an active account at
NERSC. This mailing list is automatically populated with the email addresses
associated with active NERSC accounts. In order to remove yourself from this
mailing list, you must close your account, which can be done by emailing
<accounts@nersc.gov> with your request.

