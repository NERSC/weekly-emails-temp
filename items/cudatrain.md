### CUDA Training Series Continues Next Thursday, April 16

NVIDIA is presenting a 9-part CUDA training series intended to help new and 
existing GPU programmers understand the main concepts of the CUDA platform and 
its programming model. Each part will include a 1-hour presentation and example 
exercises. The exercises are meant to reinforce the material from the 
presentation and can be completed during a 1-hour hands-on session following 
each lecture via teleconference or on your own. **This event will be held 
exclusively online.**

The fourth training in the series, on Fundamental CUDA Optimization, is a
continuation of what was learned last time about optimization strategies related
to kernel launch configurations, GPU latency hiding, global memory throughput, 
and shared memory applicability.

Following the presentation will be a hands-on session where participants can 
complete example exercises meant to reinforce the presented concepts.

Registration for part 4 closes this Thursday, April 9. For more 
information (including registration information) please see 
<https://www.nersc.gov/users/training/events/fundamental-cuda-optimization-part-2-part-4-of-9-cuda-training-series/>.

Other scheduled dates in the series:
- May 13: [5. CUDA Atomics, Reductions, and Warp Shuffle](https://www.nersc.gov/users/training/events/cuda-atomics-reductions-and-warp-shuffle-part-5-of-9-cuda-training-series/)
- June 18: 6. Managed Memory
- July 21: 7. CUDA Concurrency
