## CUDA Training Series Resumes February 19

NVIDIA is presenting a 9-part CUDA training series intended to help new and 
existing GPU programmers understand the main concepts of the CUDA platform and 
its programming model. Each part will include a 1-hour presentation and example 
exercises. The exercises are meant to reinforce the material from the 
presentation and can be completed during a 1-hour hands-on session following 
each lecture (for in-person participants) or on your own (for remote 
participants). OLCF and NERSC will both be holding in-person events for each 
part of the series.

The second training in the series, on CUDA shared memory, will take place on 
Wednesday, February 19, 2020, from 10 am to 12 pm (Pacific time). This training 
will introduce users to CUDA shared memory, and how to use it to manage data 
caches, speed up high-performance cooperative parallel algorithms, and 
facilitate global memory coalescing in cases where it would otherwise not be 
possible. Following the presentation will be a hands-on session where in-person 
participants can complete example exercises meant to reinforce the presented 
concepts.

For more information (including registration information) please see 
<https://www.nersc.gov/users/training/events/cuda-shared-memory-part-2-of-9-cuda-training-series/>

Other scheduled dates in the series:
- March 18: [3. Fundamental CUDA Optimization (Part 1)](https://www.nersc.gov/users/training/events/fundamental-cuda-optimization-part-1-part-3-of-9-cuda-training-series/)
- April 16: [4. Fundamental CUDA Optimization (Part 2)](https://www.nersc.gov/users/training/events/fundamental-cuda-optimization-part-2-part-4-of-9-cuda-training-series/)
- May 13: [5. CUDA Atomics, Reductions, and Warp Shuffle](https://www.nersc.gov/users/training/events/cuda-atomics-reductions-and-warp-shuffle-part-5-of-9-cuda-training-series/)
